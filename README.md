# ASLÂ PredictorÂ ğŸ–ï¸â†’ğŸ…°ï¸

*Realâ€‘time demo that detects a single hand in webcam video, predicts AmericanÂ SignÂ Language (ASL) letters with a RandomÂ Forest model, and types the recognised text in the browser.*

LiveÂ ğŸ‘‰ **[https://mpenalverguilera.github.io/sign-language-letters/](https://mpenalverguilera.github.io/sign-language-letters/)**

---

## âœ¨ Introduction

> **ASLÂ Predictor** is an endâ€‘toâ€‘end proofâ€‘ofâ€‘concept that shows how to build, train and serve a lightweight signâ€‘language letter recogniser:
>
> 1. **PythonÂ /Â MediaPipe** to extract 3â€‘D handÂ landmarks (21 pts)
> 2. **RandomÂ Forest** (scikitâ€‘learn) to classify 60 normalised features âœ 28 classes (Aâ€“ZÂ +Â *del*Â +Â *space*) with a **99.0â€¯% accuracy** on the test set.
> 3. **AWS Lambda (Container)** + **APIÂ Gateway** to expose `/predict`
> 4. **VanillaÂ JSÂ +Â MediaPipe Hands** frontâ€‘end published on **GitHubÂ Pages**

> **Note:** The demo is **not** intended for production use, but rather as a learning resource for building ML applications with serverless architecture.

---

## ğŸ—‚ï¸ RepositoryÂ Layout

â”œâ”€â”€ core-model/ # notebooks & scripts: dataâ€‘prep, training, evaluation

â”œâ”€â”€ lambda-app/ # backend container (Dockerfile, app, AWSÂ SAMÂ template)

â”œâ”€â”€ web-client/ # static site (index.html, hands-client.js, style.css)

â””â”€â”€ README.md # you are here


*Dataset files are **NOT** tracked. See â€œDatasetâ€ below.*

---

## ğŸ”§ Requirements

| Component      | Version | Notes                            |
| -------------- | ------- | -------------------------------- |
| Python         | 3.10    | Conda env recommended            |
| Docker         | â‰¥Â 20.10 | build Lambda container           |
| AWSÂ SAMÂ CLI    | â‰¥Â 1.110 | build/deploy Lambda              |
| Browser | Latest  | For MediaPipe Hands in frontâ€‘end        |

---

## ğŸ“¦ Dataset

- **ASL Alphabet** â€“ [https://www.kaggle.com/grassknoted/asl-alphabet](https://www.kaggle.com/grassknoted/asl-alphabet)  
  Download and unzip under `data/raw/` if you want to reproduce training.
- ~87â€¯k images â†’ **filtered** with MediaPipe (bad/no-hand frames rejected)  
  â†’ **vectorised** into 60â€‘D feature CSV (80â€¯/â€¯20 split).
- **Key Insight** ğŸ§  â€“ MediaPipe requires visible palms â†’ letters like **M** and **N** often lose samples in static images.  
  This skews the training set and increases confusion, mitigated in production using continuous video tracking.

---

## ğŸ¤– Model Training (`core-model/`)

1. `01-explore-mediapipe.ipynb`  
   Initial tests with MediaPipe Holistic to check landmark quality and image filtering.
2. `02-dataset-filtering-analysis.py`  
   Analyse distribution after sample rejection.
3. `03.1-landmark-extraction-and-normalisation.ipynb`  
   Extract and normalise 60-D features (wrist-centred, peak-to-peak scaled).
4. `03.2-landmark-extraction-and-normalisation.ipynb`  
   Adds 5 features related hand geometry (65-D).
5. `04-rf-train.py`  
   RandomisedSearchCV â†’ **99.0â€¯% accuracy**.

> The 60â€‘feature variant is smaller & faster than the 65â€‘D version, with no accuracy gain.  
> Extra features aimed to improve M/N distinction were redundant due to already computed distances.

---

## â˜ï¸ Backend â€“ AWSÂ Lambda (Container)

| File/Dir                | Purpose                                                                                         |
| ----------------------- | ------------------------------------------------------------------------------------------------|
| `lambda-app/Dockerfile` | Builds image from `public.ecr.aws/lambda/python:3.10` with all the requirements to run the model|
| `app/main.py`           | `lambda_handler` â€“ loads model, normalises input, returns JSON `{prediction, confidence}`       |
| `template.yaml`         | SAM stack (LambdaÂ +Â APIÂ GatewayÂ +Â CORS)                                                         |

*(CORS restricted to `https://mpenalverguilera.github.io` in production)*

---

## ğŸŒ Frontâ€‘end â€“ GitHubÂ PagesÂ Demo
The browser:

- Captures video with `MediaPipe Hands`
- Extracts hand landmarks (21 pts)
- Throttles `/predict` calls if hand is **stable** (`Î”L2 â‰¤ threshold` in a window of 0.5â€¯s)
- Only accepts predictions with `confidence â‰¥ 0.75`
- Appends valid letters to text box
- Handles `_space_` and `_del_` gestures
- Avoids duplicates in short timeframes

> This logic prevents overloading the backend and ensures only consistent signs are typed.

---

## ğŸ“ LessonsÂ Learned

- âœ… **Lambda Containers** are ideal for lightweight ML demos â€” serverless, scalable, costâ€‘effective.
- âœ… **Random Forest vs. CNN**: scikit-learn RandomForest is *sufficient* (99â€¯%), lightweight, interpretable.
    However, a CNN could improve accuracy when the hand is not as strict as its in the dataset.
- âš ï¸ **Pose rigidity**: model fails with casual gestures; this could be handled by relaxing the model (e.g. `min_samples_leaf` or `min_samples_spilt`)
    or training with more natural hand poses.
---

## ğŸ“œ Licence & Credits

- Dataset: â€œASL Alphabetâ€ Â© [grassknoted](https://github.com/grassknoted) (Kaggle)
- MediaPipe Hands Â©Â Google Research, openâ€‘sourced under ApacheÂ 2.0
- Code licensed under **MIT** Â©Â MarcÂ PeÃ±alverÂ Guilera.  
  Feel free to fork, share, learn.
